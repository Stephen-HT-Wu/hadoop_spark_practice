# java
sudo apt-get update
sudo apt-get install default-jdk
java -version

# SSH without password
sudo apt-get install ssh
sudo apt-get install rsync


# hadoop 2.10.0 and Spark 2.4.5
wget http://ftp.mirror.tw/pub/apache/hadoop/common/hadoop-2.10.0/hadoop-2.10.0.tar.gz
tar -zxvf hadoop-2.10.0.tar.gz
sudo mv hadoop-2.10.0 /usr/local/hadoop
ll /usr/local/hadoop/

https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-common/SingleCluster.html
# in ~/.bashrc add
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
# then
source ~/.bashrc
# and test  
/usr/local/hadoop/bin/hadoop

etc/hadoop/core-site.xml:
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

etc/hadoop/hdfs-site.xml:
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

# Setup passphraseless ssh
 ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
 chmod 0600 ~/.ssh/authorized_keys

#add JAVA_HOME manually
vim hadoop-env.sh
JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# test 
cd /usr/local/hadoop
./sbin/start-dfs.sh
./sbin/stop-dfs.sh

bin/hdfs dfs -mkdir /user
bin/hdfs dfs -mkdir /user/stephen

mkdir input
cp etc/hadoop/*.xml input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.0.jar grep input output 'dfs[a-z.]+'
bin/hdfs dfs -put etc/hadoop input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.0.jar grep input output 'dfs[a-z.]+'
bin/hdfs dfs -get output output
cat output/*

#YARN
#etc/hadoop/mapred-site.xml
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
</property>

#etc/hadoop/yarn-site.xml
 <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
 </property>

sbin/start-yarn.sh

http://localhost:8088/

sbin/stop-yarn.sh



